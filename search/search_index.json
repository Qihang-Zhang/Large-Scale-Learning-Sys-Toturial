{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"Homepage","text":"css add shadow <p>Hi, this is Qihang\ud83d\udc4b. I\u2019m documenting my learning process in this tutorial. This tutorial will focus both on the latest advancements in learning algorithms and engineering techniques.</p>"},{"location":"Tutorial/motivation-en.html","title":"Motivation","text":"<p>The idea of writing a Learning Blog has been on my mind for a long time. I have always been inspired by Lil's Log, and recently, after seeing my friend\u2019s work on Awesome-ML-SYS-Tutorial, I decided to start my own blog to organize my learnings, thoughts, and reflections, while also sharing insights and sparking discussions with others.</p> <p>Although it has been quite some time since the emergence of GPT-3.5, it was only recently that I realized that research on Large-Scale Learning Systems has evolved into a paradigm fundamentally different from the Deep Learning era that began with AlexNet in 2012. The efforts required in this field can no longer be simply summarized by a single Scaling Law. As a System, LLMs' System Design has become a highly dynamic and rapidly evolving research area.</p> <p>The recent advancements from Deepseek have made this shift even more tangible for me. Whether it is DeepseekV3 leveraging DeepSeekMoE or the innovations in NSA, it is evident that LLM design is increasingly adapting to GPU-specific constraints, such as communication costs, to expand context windows or scale up model capacity within limited computational resources. These key developments strongly reflect System Design principles. At this point in time, the skill set required for research has undergone a significant transformation compared to the era of small-scale Learning Systems.</p> <p>Meanwhile, with the rise of Cursor and other Agentic Tools, it has become increasingly clear that the performance of LLMs is often influenced by the way they interact with users. This characteristic was never seen in the pre-LLM era of Learning Systems. Interaction design is just one example, but overall, in Large-Scale Learning Systems, both their fundamental characteristics and research focus are undergoing significant changes. Years from now, we may look back and find that the paradigm shift between LLMs and Deep Learning is as profound as the transition from Deep Learning to the Machine Learning era.</p> <p>From these perspectives, I believe that the skill set and research focus for Large-Scale Learning Systems are rapidly evolving. This is why I want to write a tutorial on Large-Scale Learning Systems, to systematically organize and share my thought process.</p> <p>Qihang 2025-02-28</p>"},{"location":"Tutorial/motivation-zh.html","title":"Motivation","text":"<p>\u200b\u8981\u200b\u5199\u200b\u4e00\u4e2a\u200bLeanring Blog\u200b\u7684\u200b\u60f3\u6cd5\u200b\u5b9e\u5728\u200b\u662f\u200b\u7531\u6765\u5df2\u4e45\u200b\uff0c\u200b\u4e00\u76f4\u200b\u4ee5\u6765\u200b\u88ab\u200bLil's Log\u200b\u6240\u200b\u6fc0\u52b1\u200b\uff0c\u200b\u8fd1\u65e5\u200b\u53c8\u200b\u770b\u5230\u200b\u670b\u53cb\u200b\u5199\u200b\u7684\u200bAwesome-ML-SYS-Tutorial\uff0c\u200b\u9042\u200b\u51b3\u5b9a\u200b\u5e94\u8be5\u200b\u5f00\u59cb\u200b\u5199\u200b\u81ea\u5df1\u200b\u7684\u200bblog\uff0c\u200b\u6765\u200b\u6574\u7406\u200b\u6240\u5b66\u200b\u6240\u601d\u200b\u6240\u200b\u60f3\u200b\uff0c\u200b\u4e5f\u200b\u629b\u7816\u5f15\u7389\u200b\uff0c\u200b\u4ee5\u98e8\u8bfb\u8005\u200b\u3002</p> <p>\u200b\u867d\u7136\u200b\u81ea\u200b GPT-3.5 \u200b\u8bde\u751f\u200b\u4ee5\u6765\u200b\u5df2\u7ecf\u200b\u8fc7\u53bb\u200b\u4e86\u200b\u76f8\u5f53\u200b\u7684\u200b\u65f6\u95f4\u200b\uff0c\u200b\u4f46\u200b\u76f4\u5230\u200b\u6700\u8fd1\u200b\u6211\u200b\u624d\u200b\u540e\u77e5\u540e\u89c9\u200b\u5730\u200b\u610f\u8bc6\u200b\u5230\u200b\uff0cLarge-Scale Learning System\u200b\u7684\u200b\u7814\u7a76\u200b\u65e9\u200b\u5df2\u7ecf\u200b\u53d1\u5c55\u200b\u6210\u200b\u4e86\u200b\u4e00\u79cd\u200b\u4e0e\u200b2012\u200b\u5e74\u200b\u4ee5\u6765\u200bAlex Net\u200b\u4e3a\u200b\u4ee3\u8868\u200b\u7684\u200bDeep Learning\u200b\u5b8c\u5168\u200b\u4e0d\u540c\u200b\u7684\u200b\u7814\u7a76\u200b\u8303\u5f0f\u200b\u3002\u200b\u5176\u4e2d\u200b\u6240\u200b\u9700\u200b\u7684\u200b\u52aa\u529b\u200b\uff0c\u200b\u8fdc\u975e\u200b\u4e00\u53e5\u200b Scaling Law \u200b\u5c31\u200b\u80fd\u200b\u8f7b\u63cf\u6de1\u5199\u200b\u7684\u200b\u6982\u62ec\u200b\u3002\u200b\u4f5c\u4e3a\u200b\u4e00\u4e2a\u200bSystem\uff0cLLMs \u200b\u7684\u200bSystem Design\u200b\u65e9\u5df2\u200b\u6210\u4e3a\u200b\u4e00\u4e2a\u200b\u9ad8\u5ea6\u200b\u6d3b\u8dc3\u200b\u3001\u200b\u65e5\u65b0\u6708\u5f02\u200b\u7684\u200b\u7814\u7a76\u200b\u9886\u57df\u200b\u3002</p> <p>\u200b\u6700\u8fd1\u200bDeepseek\u200b\u7684\u200b\u8fdb\u5c55\u200b\u8ba9\u200b\u6211\u200b\u66f4\u52a0\u200b\u76f4\u89c2\u200b\u5730\u200b\u611f\u53d7\u200b\u5230\u200b\u8fd9\u200b\u4e00\u200b\u53d8\u5316\u200b\u3002\u200b\u65e0\u8bba\u662f\u200bDeepseekV3\u200b\u91c7\u7528\u200b\u7684\u200bDeepSeekMoE\u200b\u8fd8\u662f\u200bNSA\uff0c\u200b\u90fd\u200b\u8868\u660e\u200b\u5728\u200bLLMs\u200b\u7684\u200b\u8bbe\u8ba1\u200b\u8fc7\u7a0b\u200b\u4e2d\u200b\uff0c\u200b\u4e3a\u4e86\u200b\u8fdb\u4e00\u6b65\u200b\u6269\u5927\u200bcontext window\u200b\u6216\u200b\u5728\u200b\u6709\u9650\u200b\u7684\u200b\u8ba1\u7b97\u8d44\u6e90\u200b\u4e0b\u200bscale up\u200b\u6a21\u578b\u200b\u5bb9\u91cf\u200b\uff0c\u200b\u8bb8\u591a\u200b\u8bbe\u8ba1\u200b\u90fd\u200b\u5728\u200b\u9002\u5e94\u200bGPU\u200b\u7684\u200b\u7279\u6027\u200b\uff0c\u200b\u5982\u200b\u901a\u4fe1\u200b\u6210\u672c\u200b\u7b49\u200b\u3002\u200b\u8fd9\u4e9b\u200b\u5173\u952e\u200b\u8fdb\u5c55\u200b\u5df2\u7ecf\u200b\u5c55\u73b0\u51fa\u200b\u5f3a\u70c8\u200b\u7684\u200bSystem\u200b\u7684\u200b\u5473\u9053\u200b\u3002\u200b\u7ad9\u200b\u5728\u200b\u5f53\u4e0b\u200b\u8fd9\u4e2a\u200b\u8282\u70b9\u200b\uff0c\u200b\u6211\u4eec\u200b\u5728\u200b\u7814\u7a76\u200b\u4e2d\u6240\u200b\u9700\u200b\u7684\u200bskill set\uff0c\u200b\u4e0e\u200b\u5c0f\u89c4\u6a21\u200bLearning System\u200b\u7684\u200b\u65f6\u4ee3\u200b\u76f8\u6bd4\u200b\uff0c\u200b\u5df2\u7ecf\u200b\u53d1\u751f\u200b\u4e86\u200b\u5de8\u5927\u53d8\u5316\u200b\u3002</p> <p>\u200b\u4e0e\u6b64\u540c\u65f6\u200b\uff0c\u200b\u968f\u7740\u200b Cursor\u200b\u7b49\u200b Agentic Tools\u200b\u7684\u200b\u5174\u8d77\u200b\uff0c\u200b\u53ef\u4ee5\u200b\u660e\u663e\u200b\u611f\u53d7\u200b\u5230\u200bLLMs\u200b\u7684\u200b\u8868\u73b0\u200b\u5728\u200b\u5f88\u5927\u200b\u7a0b\u5ea6\u200b\u4e0a\u200b\u53d6\u51b3\u4e8e\u200b\u4e0e\u5176\u200b\u4ea4\u4e92\u200b\u7684\u200b\u65b9\u5f0f\u200b\u3002\u200b\u8fd9\u4e00\u200b\u7279\u6027\u200b\u5728\u200bLLMs\u200b\u4e4b\u524d\u200b\u7684\u200bLearning System\u200b\u65f6\u4ee3\u200b\u4ece\u672a\u200b\u5c55\u73b0\u200b\u8fc7\u200b\u3002\u200b\u4ea4\u4e92\u65b9\u5f0f\u200b\u53ea\u662f\u200b\u5176\u4e2d\u200b\u4e00\u4e2a\u200b\u5f88\u200b\u76f4\u89c2\u200b\u7684\u200b\u4f8b\u5b50\u200b\uff0c\u200b\u603b\u4f53\u800c\u8a00\u200b\uff0c\u200b\u5728\u200b\u66f4\u200b\u5927\u89c4\u6a21\u200b\u7684\u200bLearning System\u200b\u4e2d\u200b\uff0c\u200b\u5176\u200b\u7279\u6027\u200b\u548c\u200b\u7814\u7a76\u200b\u91cd\u5fc3\u200b\u6b63\u5728\u200b\u53d1\u751f\u200b\u663e\u8457\u200b\u7684\u200b\u53d8\u5316\u200b\u3002\u200b\u6216\u200b\u8bb8\u591a\u5e74\u200b\u540e\u200b\u56de\u987e\u200b\u5f53\u4e0b\u200b\uff0c\u200b\u6211\u4eec\u200b\u4f1a\u200b\u53d1\u73b0\u200bLLMs\u200b\u7684\u200b\u7814\u7a76\u200b\u8303\u5f0f\u200b\u4e0e\u200b\u82e5\u5e72\u5e74\u200b\u524d\u200b\u7684\u200bDeep Learning\u200b\u4e4b\u95f4\u200b\u7684\u200b\u5dee\u5f02\u200b\uff0c\u200b\u53ef\u80fd\u200b\u4f1a\u200b\u50cf\u200bDeep Learning\u200b\u4e0e\u200bMachine Learning\u200b\u65f6\u4ee3\u200b\u7684\u200b\u5dee\u5f02\u200b\u4e00\u6837\u200b\u5de8\u5927\u200b\u3002</p> <p>\u200b\u4ece\u200b\u8fd9\u200b\u4e24\u4e2a\u200b\u89d2\u5ea6\u200b\u6765\u770b\u200b\uff0c\u200b\u6211\u200b\u8ba4\u4e3a\u200bLarge-Scale Learning System\u200b\u7684\u200b\u7814\u7a76\u6240\u200b\u9700\u200b\u7684\u200bskill set\u200b\u548c\u200b\u7814\u7a76\u200b\u91cd\u5fc3\u200b\u90fd\u200b\u5728\u200b\u5feb\u901f\u200b\u53d8\u5316\u200b\u3002\u200b\u8fd9\u200b\u4e5f\u200b\u662f\u200b\u6211\u200b\u5e0c\u671b\u200b\u64b0\u5199\u200b\u4e00\u4efd\u200b\u5173\u4e8e\u200bLarge-Scale Learning System\u200b\u7684\u200btutorial\uff0c\u200b\u4ee5\u200b\u6574\u7406\u200b\u81ea\u5df1\u200b\u7684\u200b\u601d\u8003\u200b\u8f68\u8ff9\u200b\u7684\u200b\u539f\u56e0\u200b\u3002</p> <p>Qihang 2025-02-28</p>"},{"location":"Tutorial/2025/02/27/Example.html","title":"Example","text":"<p>This is an example post.</p>"},{"location":"Tutorial/2025/02/27/Example.html#overview","title":"Overview","text":"<ul> <li>Example<ul> <li>Overview</li> <li>Section1<ul> <li>Subsection1</li> </ul> </li> <li>Section2</li> </ul> </li> </ul>"},{"location":"Tutorial/2025/02/27/Example.html#section1","title":"Section1","text":""},{"location":"Tutorial/2025/02/27/Example.html#subsection1","title":"Subsection1","text":"<p>Hello, world!</p>"},{"location":"Tutorial/2025/02/27/Example.html#section2","title":"Section2","text":""},{"location":"Tutorial/2025/03/02/sglang%20profiling.html","title":"SGLang profiling","text":"<p>This post is about my learning experience on profiling SGLang.</p> <p>Reference:</p> <ul> <li> <p>\u200b\u5982\u4f55\u200b\u914d\u7f6e\u200b\u4e00\u53f0\u200b\u723d\u5feb\u200b\u7684\u200b\u5f00\u53d1\u200b\u673a\u5668\u200b</p> </li> <li> <p>sglang doc</p> </li> <li> <p>sglang benchmark</p> </li> </ul>"},{"location":"Tutorial/2025/03/04/Sparsity%20In%20LLMs.html","title":"Sparsity in LLMs","text":"<p>This post is about my learning experience on sparsity in LLMs.</p>"},{"location":"Tutorial/2025/03/04/Sparsity%20In%20LLMs.html#overview","title":"Overview","text":"<ul> <li>Sparsity in LLMs<ul> <li>Overview</li> <li>DeepSeek-Series<ul> <li>Multi-Head Latent Attention<ul> <li>Standard Multi-Head Attention</li> <li>Low-Rank Key-Value Joint Compression</li> </ul> </li> <li>Deepseek MoE<ul> <li>Basic Architecture</li> </ul> </li> </ul> </li> </ul> </li> </ul>"},{"location":"Tutorial/2025/03/04/Sparsity%20In%20LLMs.html#deepseek-series","title":"DeepSeek-Series","text":""},{"location":"Tutorial/2025/03/04/Sparsity%20In%20LLMs.html#multi-head-latent-attention","title":"Multi-Head Latent Attention","text":""},{"location":"Tutorial/2025/03/04/Sparsity%20In%20LLMs.html#standard-multi-head-attention","title":"Standard Multi-Head Attention","text":"<p>\\(d\\) is the embedding dimension of a token, \\(n_h\\) is the number of attention heads, \\(d_h\\) is the dimension per head, and \\(h_t \\in \\mathbb{R}^d\\) is the attention input of the \\(t\\)-th token at an attention layer. Standard MHA first produces \\(q_t, k_t, v_t \\in \\mathbb{R}^{d_h n_h}\\) through three matrices \\(W^Q, W^K, W^V \\in \\mathbb{R}^{d_h n_h \\times d}\\), respectively:</p> \\[ q_t = W^Q h_t, \\] \\[ k_t = W^K h_t, \\] \\[ v_t = W^V h_t. \\] <p>Then, \\(q_t, k_t, v_t\\) will be sliced into \\(n_h\\) heads for the multi-head attention computation:</p> \\[ [q_{t,1}; q_{t,2}; \\dots; q_{t,n_h}] = q_t, \\] \\[ [k_{t,1}; k_{t,2}; \\dots; k_{t,n_h}] = k_t, \\] \\[ [v_{t,1}; v_{t,2}; \\dots; v_{t,n_h}] = v_t. \\] \\[ o_{t,i} = \\sum_{j=1}^{t} \\text{Softmax}_j \\left( \\frac{q_{t,i}^T k_{j,i}}{\\sqrt{d_h}} \\right) v_{j,i}, \\] \\[ u_t = W^O [o_{t,1}; o_{t,2}; \\dots; o_{t,n_h}]. \\] <p>where \\(q_{t,i}, k_{t,i}, v_{t,i} \\in \\mathbb{R}^{d_h}\\) denote the query, key, and value of the \\(i\\)-th attention head, respectively; \\(W^O \\in \\mathbb{R}^{d \\times d_h n_h}\\) denotes the output projection matrix. During inference, all keys and values need to be cached to accelerate inference, so MHA needs to cache \\(2 n_h d_h\\) elements for each token. In model deployment, this heavy KV cache is a large bottleneck that limits the maximum batch size and sequence length.</p>"},{"location":"Tutorial/2025/03/04/Sparsity%20In%20LLMs.html#low-rank-key-value-joint-compression","title":"Low-Rank Key-Value Joint Compression","text":"<p>The core of MLA is the low-rank joint compression for keys and values to reduce KV cache:</p> \\[ \\mathbf{c}_t^{KV} = W^{DKV} \\mathbf{h}_t,  \\] \\[ \\mathbf{k}_t^C = W^{UK} \\mathbf{c}_t^{KV},  \\] \\[ \\mathbf{v}_t^C = W^{UV} \\mathbf{c}_t^{KV},  \\] <p>where \\(\\mathbf{c}_t^{KV} \\in \\mathbb{R}^{d_c}\\) is the compressed latent vector for keys and values; \\(d_c (\\ll d_h n_h)\\) denotes the KV compression dimension; \\(W^{DKV} \\in \\mathbb{R}^{d_c \\times d}\\) is the down-projection matrix; and \\(W^{UK}, W^{UV} \\in \\mathbb{R}^{d_h n_h \\times d_c}\\) are the up-projection matrices for keys and values, respectively. </p> <p>During inference, MLA only needs to cache \\(\\mathbf{c}_t^{KV}\\), so its KV cache has only \\(d_c l\\) elements, where \\(l\\) denotes the number of layers.</p> <p>In addition, during inference, since \\(W^{UK}\\) can be absorbed into \\(W^Q\\), and \\(W^{UV}\\) can be absorbed into \\(W^O\\), we even do not need to compute keys and values out for attention. </p> <p>Moreover, in order to reduce the activation memory during training, we also perform low-rank compression for the queries, even if it cannot reduce the KV cache:</p> \\[ \\mathbf{c}_t^Q = W^{DQ} \\mathbf{h}_t,  \\] \\[ \\mathbf{q}_t^C = W^{UQ} \\mathbf{c}_t^Q,  \\] <p>where \\(\\mathbf{c}_t^Q \\in \\mathbb{R}^{d_c'}\\) is the compressed latent vector for queries; \\(d_c' (\\ll d_h n_h)\\) denotes the query compression dimension; and \\(W^{DQ} \\in \\mathbb{R}^{d_c' \\times d}\\), \\(W^{UQ} \\in \\mathbb{R}^{d_h n_h \\times d_c'}\\) are the down-projection and up-projection matrices for queries, respectively.</p>"},{"location":"Tutorial/2025/03/04/Sparsity%20In%20LLMs.html#deepseek-moe","title":"Deepseek MoE","text":""},{"location":"Tutorial/2025/03/04/Sparsity%20In%20LLMs.html#basic-architecture","title":"Basic Architecture","text":"<p>DeepSeekMoE has two key ideas: segmenting experts into finer granularity for higher expert specialization and more accurate knowledge acquisition, and isolating some shared experts for mitigating knowledge redundancy among routed experts. With the same number of activated and total expert parameters, DeepSeekMoE can outperform conventional MoE architectures like GShard by a large margin.</p> <p>Let \\(\\mathbf{u}_t\\) be the FFN input of the \\(t\\)-th token, we compute the FFN output \\(\\mathbf{h}_t'\\) as follows:</p> \\[ \\mathbf{h}_t' = \\mathbf{u}_t + \\sum_{i=1}^{N_s} \\text{FFN}_i^{(s)}(\\mathbf{u}_t) + \\sum_{i=1}^{N_r} g_{i,t} \\text{FFN}_i^{(r)}(\\mathbf{u}_t), \\] \\[ g_{i,t} = \\begin{cases}   s_{i,t}, &amp; s_{i,t} \\in \\text{Topk}({s_{j,t} | 1 \\leq j \\leq N_r}, K_r), \\\\  0, &amp; \\text{otherwise},  \\end{cases} \\] \\[  s_{i,t} = \\text{Softmax}_i(\\mathbf{u}_t^T \\mathbf{e}_i), \\] <p>where \\(N_s\\) and \\(N_r\\) denote the numbers of shared experts and routed experts, respectively; \\(\\text{FFN}_i^{(s)}(\\cdot)\\) and \\(\\text{FFN}_i^{(r)}(\\cdot)\\) denote the \\(i\\)-th shared expert and the \\(i\\)-th routed expert, respectively; \\(K_r\\) denotes the number of activated routed experts; \\(g_{i,t}\\) is the gate value for the \\(i\\)-th expert; \\(s_{i,t}\\) is the token-to-expert affinity; </p> <p>\\(\\mathbf{e}_i\\) is the centroid of the \\(i\\)-th routed expert in this layer;</p> <p>and \\(\\text{Topk}(\\cdot, K)\\) denotes the set comprising \\(K\\) highest scores among the affinity scores calculated for the \\(t\\)-th token and all routed experts.</p> <p>Reference:</p>"},{"location":"Tutorial/archive/2025.html","title":"2025","text":""},{"location":"Tutorial/category/sparsity.html","title":"Sparsity","text":""},{"location":"Tutorial/category/in-progress.html","title":"In progress","text":""},{"location":"Tutorial/category/profiling.html","title":"Profiling","text":""},{"location":"Tutorial/category/random-thoughts.html","title":"Random Thoughts","text":""}]}